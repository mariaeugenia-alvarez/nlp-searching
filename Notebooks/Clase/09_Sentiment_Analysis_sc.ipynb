{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fv_sftqpG1k"
      },
      "source": [
        "# Librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt\n",
        "!pip install utils.py"
      ],
      "metadata": {
        "id": "wBXVY_GGBB6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unzip\n",
        "!unzip corpusCine.zip"
      ],
      "metadata": {
        "id": "NkO_kvJYBEvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "kJPU9pbs-SYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "\n",
        "!pip install -U scikit-learn\n",
        "\n",
        "import sklearn\n",
        "\n",
        "sklearn.__version__"
      ],
      "metadata": {
        "id": "39yPKxFM-G-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HntXJS6pG1n"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install pandas\n",
        "\n",
        "\n",
        "!pip install numpy\n",
        "\n",
        "from utils import load_cinema_reviews\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY4D8YYapG1o"
      },
      "source": [
        "# Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1M1lJ7EpG1p"
      },
      "outputs": [],
      "source": [
        "# Path al directorio donde tenemos los datasets con las reviews\n",
        "# ¡Descomprimir antes!\n",
        "datasets_path = './'\n",
        "corpus_cine_folder = 'corpusCine'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtkshSOdpG1p"
      },
      "outputs": [],
      "source": [
        "\n",
        "reviews_dict = load_cinema_reviews(datasets_path, corpus_cine_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_dict.get(10)"
      ],
      "metadata": {
        "id": "NYZMd0SHBHmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oan26tm0pG1q"
      },
      "outputs": [],
      "source": [
        "reviews_text = []\n",
        "reviews_sentiment = []\n",
        "\n",
        "for review in reviews_dict.values():\n",
        "    reviews_text.append(review.get('review_text'))\n",
        "    reviews_sentiment.append(review.get('sentiment'))\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'review': reviews_text,\n",
        "    'sentiment': reviews_sentiment\n",
        "})\n",
        "df.dropna(subset=['review', 'sentiment'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0LCJlYg1yL5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1B-zu9IpG1r"
      },
      "outputs": [],
      "source": [
        "def label_sentiment(row):\n",
        "    if int(row['sentiment']) < 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WdOCPhZbpG1r"
      },
      "outputs": [],
      "source": [
        "df['sentiment_label'] = df.apply(lambda row: label_sentiment(row), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "SdGDb1L-yOy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5SLOo1ZpG1r"
      },
      "source": [
        "# Separamos en conjunto de train y test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j_SwxwopG1s"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review'],\n",
        "    df['sentiment_label'],\n",
        "    train_size=0.75,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.iloc[:10]"
      ],
      "metadata": {
        "id": "G3b9UZMjyzL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.iloc[:10]"
      ],
      "metadata": {
        "id": "yJmCKKfjy2g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkpFgdWhpG1s"
      },
      "source": [
        "# Extracción de features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = TfidfVectorizer(\n",
        "    max_df=0.95,\n",
        "    min_df=5,\n",
        "    max_features=2500,\n",
        "    strip_accents='ascii',\n",
        "    ngram_range=(1, 1)\n",
        ")\n",
        "cv.fit(X_train)"
      ],
      "metadata": {
        "id": "qtzfQeSDy63Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(cv.vocabulary_.items())[:20])"
      ],
      "metadata": {
        "id": "korU-m2Ky9kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(cv.vocabulary_))"
      ],
      "metadata": {
        "id": "8jtAeTAczBHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbtvUpjZpG1t"
      },
      "source": [
        "## TF-IDF scores del training set y test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIIl_icspG1t"
      },
      "outputs": [],
      "source": [
        "X_train_ = cv.transform(X_train)\n",
        "X_test_ = cv.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3JultgCpG1t"
      },
      "source": [
        "## Score IDF de algunas palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHNtzO99pG1t"
      },
      "outputs": [],
      "source": [
        "words_example = [\n",
        "    'cine',\n",
        "    'pelicula',\n",
        "    'muy',\n",
        "    'y',\n",
        "    'de',\n",
        "    'que',\n",
        "    'potter',\n",
        "    'mala',\n",
        "    'aburrimiento',\n",
        "    'sorprendente',\n",
        "    'aburrir',\n",
        "    'ausdf'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_idf = dict(zip(cv.get_feature_names_out(), cv.idf_))\n",
        "\n",
        "print('{0:20}{1:20}'.format('Palabra', 'IDF'))\n",
        "for word in words_example:\n",
        "    if word not in vocab_idf:\n",
        "        print('{0:20}{1:20}'.format(word, 'OOV'))\n",
        "    else:\n",
        "        print('{0:20}{1:2.3f}'.format(word, vocab_idf[word]))"
      ],
      "metadata": {
        "id": "vs0cP49RzFrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooUJ05G1pG1t"
      },
      "source": [
        "## Palabras con el TF-IDF en alguna review"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0, len(X_train))\n",
        "print('ID: {}'.format(i))\n",
        "print('Sentiment: {}'.format(y_train.iloc[i]))\n",
        "print('Review: {}'.format(X_train.iloc[i]))"
      ],
      "metadata": {
        "id": "3wYrfoaezJPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vector = X_train_[i]\n",
        "df_tfidf = pd.DataFrame(doc_vector.T.todense(), index=cv.get_feature_names_out(), columns=['tfidf'])\n",
        "df_tfidf = df_tfidf[df_tfidf['tfidf'] > 0]\n",
        "\n",
        "top_n = 10\n",
        "print('Top {} words with highest TF_IDF in the review {}:\\n{}'.format(top_n, i, df_tfidf.sort_values(by=[\"tfidf\"],ascending=False)[:top_n]))\n",
        "print('\\nTop {} words with lowest TF_IDF in the review {}:\\n{}'.format(top_n, i, df_tfidf.sort_values(by=[\"tfidf\"],ascending=False)[-top_n:]))"
      ],
      "metadata": {
        "id": "yqyIGlc8zLpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOZGJn36pG1u"
      },
      "source": [
        "## $\\chi²$ scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the chi-squared score for each word in the training set and show the values\n",
        "i = 15\n",
        "\n",
        "chi2score = chi2(X_train_, y_train)[0]\n",
        "scores = list(zip(cv.get_feature_names_out(), chi2score))\n",
        "sorted_scores = sorted(scores, key=lambda x:x[1])\n",
        "topchi2 = list(zip(*sorted_scores[-i:]))\n",
        "x = range(len(topchi2[1]))\n",
        "labels = topchi2[0]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(x,topchi2[1], align='center', alpha=0.5)\n",
        "plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)\n",
        "plt.yticks(x, labels, fontsize=12)\n",
        "plt.xlabel('$\\chi^2$', fontsize=26)\n",
        "plt.ylabel('word', fontsize=16)\n",
        "plt.title('Top {} $\\chi^2$ score for each word in the training set'.format(i), fontsize=20)\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "84sELpCbzRjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES65lfuepG1u"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_params = [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000]\n",
        "\n",
        "train_acc = list()\n",
        "test_acc = list()\n",
        "for c in c_params:\n",
        "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
        "    lr.fit(X_train_, y_train)\n",
        "\n",
        "    train_predict = lr.predict(X_train_)\n",
        "    test_predict = lr.predict(X_test_)\n",
        "\n",
        "    print (\"Accuracy for C={}: {}\".format(c, accuracy_score(y_test, test_predict)))\n",
        "\n",
        "    train_acc.append(accuracy_score(y_train, train_predict))\n",
        "    test_acc.append(accuracy_score(y_test, test_predict))"
      ],
      "metadata": {
        "id": "m1A7A81_zVW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1u49-JgpG1u"
      },
      "source": [
        "# Algunas métricas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Confussion matrix:\\n{}'.format(confusion_matrix(y_test, test_predict)))\n",
        "print('\\nClassification report:\\n{}'.format(classification_report(y_test, test_predict)))\n",
        "print('Accuracy score:{}'.format(accuracy_score(y_test, test_predict)))"
      ],
      "metadata": {
        "id": "upM8IREh0X7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_acc, label='train')\n",
        "plt.plot(test_acc, label='test')\n",
        "plt.axvline(np.argmax(test_acc), c='g', ls='--', alpha=0.8)\n",
        "plt.title('Accuracy evolution for different C values')\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xticks(list(range(len(c_params))), c_params)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z7MVJjSq0bRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, thresholds = precision_recall_curve(y_test, test_predict)"
      ],
      "metadata": {
        "id": "z5Kvexf50eFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeLNo9ompG1u"
      },
      "outputs": [],
      "source": [
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.xlabel(\"Decision Threshold\")\n",
        "    plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_precision_recall_vs_threshold(p, r, thresholds)"
      ],
      "metadata": {
        "id": "3-8Qqo3h0iyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ezx70d9pG1v"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-PUIHs1_s2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txs32QR_pG1v"
      },
      "outputs": [],
      "source": [
        "def predict_review_sentiment(review_index, model):\n",
        "    print('Actual sentiment: {}'.format(df.iloc[review_index]['sentiment_label']))\n",
        "    r = df.iloc[review_index]['review']\n",
        "    print('Prediction: {}'.format(lr.predict(cv.transform([r]))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i in random.sample(range(0, len(df)), 5):\n",
        "    print('\\nReview no. {}'.format(i))\n",
        "    predict_review_sentiment(i, lr)"
      ],
      "metadata": {
        "id": "4kX_68B50zgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8z9IW9frz7a-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}