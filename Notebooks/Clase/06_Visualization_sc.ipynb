{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vVmey4XdJJl"
      },
      "source": [
        "# Visualización\n",
        "\n",
        "Cuando trabajamos con imágenes, datos numéricos, variables categóricas, series temporales... es sencillo imaginar posibles visualizaciones para representar las distribuciones de los datos, algunas estadísticas, etc.\n",
        "\n",
        "Cuando trabajamos con textos quizá no es tan intuitivo, ¿o si?\n",
        "\n",
        "A continuación se muestran algunos ejemplos de visualización de datos cuando trabajamos en NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8hqTD-2dJJo"
      },
      "source": [
        "# Frecuencia de palabras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt\n",
        "!pip install unzip"
      ],
      "metadata": {
        "id": "iah5htvT__D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip inaugural.zip"
      ],
      "metadata": {
        "id": "5s7KhTDRACaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaJEojOwdJJo"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW-x62HxdJJp"
      },
      "outputs": [],
      "source": [
        "# Prefacio del NLTK book\n",
        "text = 'This is a book about Natural Language Processing. By \"natural language\" we mean a language that is used for everyday communication by humans; languages like English, Hindi or Portuguese. In contrast to artificial languages such as programming languages and mathematical notations, natural languages have evolved as they pass from generation to generation, and are hard to pin down with explicit rules. We will take Natural Language Processing — or NLP for short — in a wide sense to cover any kind of computer manipulation of natural language. At one extreme, it could be as simple as counting word frequencies to compare different writing styles. At the other extreme, NLP involves \"understanding\" complete human utterances, at least to the extent of being able to give useful responses to them. Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society. This book provides a highly accessible introduction to the field of NLP. It can be used for individual study or as the textbook for a course on natural language processing or computational linguistics, or as a supplement to courses in artificial intelligence, text mining, or corpus linguistics. The book is intensely practical, containing hundreds of fully-worked examples and graded exercises. The book is based on the Python programming language together with an open source library called the Natural Language Toolkit (NLTK). NLTK includes extensive software, data, and documentation, all freely downloadable from http://nltk.org/. Distributions are provided for Windows, Macintosh and Unix platforms. We strongly encourage you to download Python and NLTK, and try out the examples and exercises along the way.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_g1EM7jdJJq"
      },
      "outputs": [],
      "source": [
        "words_nltk = text.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_nltk[:10]"
      ],
      "metadata": {
        "id": "8DHsHTi-nDEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CveCmpPEdJJr"
      },
      "outputs": [],
      "source": [
        "wf = Counter(words_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrBeZYLvdJJr"
      },
      "outputs": [],
      "source": [
        "wf_most_common = wf.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wf_most_common"
      ],
      "metadata": {
        "id": "P6OX2ec9nGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLh7J-F6dJJr"
      },
      "outputs": [],
      "source": [
        "words = [w[0] for w in wf_most_common]\n",
        "freqs = [w[1] for w in wf_most_common]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2YU3KOYdJJs"
      },
      "outputs": [],
      "source": [
        "freqs, words = zip(*sorted(zip(freqs, words)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.barh(words, freqs)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4WW8By0cnJyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZSApylndJJs"
      },
      "source": [
        "# ... o de n-grams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "HOUWEdnUrwt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXrmTmAedJJs"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6tcDJQUdJJs"
      },
      "outputs": [],
      "source": [
        "bigrams_ = list(ngrams(words_nltk, 2))\n",
        "trigrams_ = list(ngrams(words_nltk, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_[:10]"
      ],
      "metadata": {
        "id": "J7CHXT9hnNlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams_[:10]"
      ],
      "metadata": {
        "id": "0wZY-hd9nQu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_IDH9NwLdJJt"
      },
      "outputs": [],
      "source": [
        "bg_freq = FreqDist(bigrams_)\n",
        "tg_freq = FreqDist(trigrams_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bg_freq.most_common(10)"
      ],
      "metadata": {
        "id": "WUDBNWT8nTJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tg_freq.most_common(10)"
      ],
      "metadata": {
        "id": "vovDofdcnVY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "L-jBIYaKdJJt"
      },
      "outputs": [],
      "source": [
        "bg_freq_most_common = bg_freq.most_common(10)\n",
        "bgs_ = [str(bg[0]) for bg in bg_freq_most_common]\n",
        "bgs_f_ = [bg[1] for bg in bg_freq_most_common]\n",
        "\n",
        "tg_freq_most_common = tg_freq.most_common(10)\n",
        "tgs_ = [str(tg[0]) for tg in tg_freq_most_common]\n",
        "tgs_f_ = [tg[1] for tg in tg_freq_most_common]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbtyRSjtdJJt"
      },
      "outputs": [],
      "source": [
        "bgs_f_, bgs_ = zip(*sorted(zip(bgs_f_, bgs_)))\n",
        "tgs_f_, tgs_ = zip(*sorted(zip(tgs_f_, tgs_)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.barh(bgs_, bgs_f_)\n",
        "plt.title('Bigram frequencies')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4DA6j-TQnYbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.barh(tgs_, tgs_f_)\n",
        "plt.title('Trigram frequencies')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XnXO0uGkna5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words\n",
        "\n",
        "\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "sw = get_stop_words (language = 'en')\n",
        "\n",
        "new_text = [word for word in text.lower().split() if word in sw ]\n",
        "print(new_text)"
      ],
      "metadata": {
        "id": "rssNK3QlneTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_bigrams = list (ngrams(new_text, 2))\n",
        "new_bg_freq = FreqDist(new_bigrams)\n",
        "new_bg_freq.most_common(10)"
      ],
      "metadata": {
        "id": "-VakwYHRnhcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWDYur3LdJJu"
      },
      "source": [
        "# Word cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "dq8iPApvhgHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvyaKoyFdJJu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "id": "NFY3D5aFno-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "id": "Huu_MzxJnrbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhfJ3p38dJJu"
      },
      "outputs": [],
      "source": [
        "def plot_word_cloud(text):\n",
        "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(text))\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_list = text.lower().strip().split()\n",
        "plot_word_cloud(words_list)"
      ],
      "metadata": {
        "id": "w4TiS41Bnu3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7-H_cQEdJJu"
      },
      "source": [
        "# Dispersión léxica\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CndCQJredJJv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.draw.dispersion import dispersion_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKHlI4uHdJJv"
      },
      "outputs": [],
      "source": [
        "inaugural_folder = './inaugural'\n",
        "inaugural_paths = [os.path.join(inaugural_folder, file) for file in sorted(os.listdir(inaugural_folder)) if '.txt' in file]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inaugural_paths"
      ],
      "metadata": {
        "id": "WeQoyVfhn_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bdpXolWCdJJv"
      },
      "outputs": [],
      "source": [
        "texts = list()\n",
        "for file_path in inaugural_paths:\n",
        "    with open(file_path, mode='r', encoding='latin-1') as f:\n",
        "        texts.append(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0][:1000])"
      ],
      "metadata": {
        "id": "xA_oGom6oBzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arpLUEwCdJJv"
      },
      "outputs": [],
      "source": [
        "words = [word.lower() for text in texts for word in text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rowt7MBdJJv"
      },
      "outputs": [],
      "source": [
        "target_words = [\n",
        "    'democracy',\n",
        "    'citizens',\n",
        "    'freedom',\n",
        "    'duties',\n",
        "    'america'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 9))\n",
        "plt.style.use('default')\n",
        "dispersion_plot(words, target_words, ignore_case=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l8KG3dK5oLnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOurz2CPdJJv"
      },
      "source": [
        "## Ejemplo: Google Ngram Viewer\n",
        "\n",
        "Buscador online que permite representar en un gráfico la frecuencia anual de distintos ngrams detectados en los corpus que tiene Google disponibles para multitud de idiomas.\n",
        "\n",
        "<img src=https://images2.minutemediacdn.com/image/upload/c_fit,f_auto,fl_lossy,q_auto,w_728/v1555921104/shape/mentalfloss/screen_shot_2014-11-12_at_1.43.16_pm.png>\n",
        "\n",
        "Link: https://books.google.com/ngrams#\n",
        "\n",
        "Alguna curiosidad:\n",
        "- Artículo: _Experiments in Ngram Art_, [link](https://www.mentalfloss.com/article/60033/experiments-ngram-art)\n",
        "- TED Talk: _What we learned from 5 million books_, [link](https://www.ted.com/talks/jean_baptiste_michel_erez_lieberman_aiden_what_we_learned_from_5_million_books/up-next?language=en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXEpzq2PdJJv"
      },
      "source": [
        "# Ley de Zipf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l-BfGpodJJv"
      },
      "source": [
        "Formulada en la década de 1940 por el lingüista George Kingsley Zipf, establece que, dada una lengua, la frecuencia de aparición de las distintas palabras de su vocabulario sigue una distribución que puede aproximarse por:\n",
        "\n",
        "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/9fa76f350fe93da686890acfb9b8e3b1151b85bc>\n",
        "\n",
        "Gráfico log-log con el ranking y la frecuencia de las 10 millones de palabras más frecuentes (medido con artículos de Wikipedia) para distintos idiomas:\n",
        "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Zipf_30wiki_es_labels.png/1200px-Zipf_30wiki_es_labels.png>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eTX5nrbdJJv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.probability import FreqDist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajAWZaV7dJJw"
      },
      "outputs": [],
      "source": [
        "fd = FreqDist(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd"
      ],
      "metadata": {
        "id": "AZDEAxCYoQvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bhYnAbGdJJw"
      },
      "outputs": [],
      "source": [
        "fd = {k: v for k, v in sorted(fd.items(), key=lambda item: item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh4tdIN1dJJw"
      },
      "outputs": [],
      "source": [
        "ranks = list()\n",
        "freqs = list()\n",
        "\n",
        "for rank, word in enumerate(fd):\n",
        "    ranks.append(rank+1)\n",
        "    freqs.append(fd[word])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.loglog(ranks, freqs)\n",
        "plt.xlabel('Rank')\n",
        "plt.ylabel('Freq')\n",
        "plt.title('Log-Log rank-freq chart')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GHMFANJ3oUBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}