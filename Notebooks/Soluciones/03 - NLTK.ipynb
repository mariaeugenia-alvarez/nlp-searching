{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"qPtoSlCAFhKi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting unzip\n","  Downloading unzip-1.0.0.tar.gz (704 bytes)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unzip\n","  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1281 sha256=20800465ec2794cfdda941d2f4e6030c4baba76d4060cd5c06a36a438e2dd8c3\n","  Stored in directory: /root/.cache/pip/wheels/fb/5b/81/0f3e1e533b52883f88ab978178c15627a4fce4c13f74911dce\n","Successfully built unzip\n","Installing collected packages: unzip\n","Successfully installed unzip-1.0.0\n","Collecting gensim (from -r requirements.txt (line 1))\n","  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (6.17.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (7.34.0)\n","Collecting jellyfish (from -r requirements.txt (line 4))\n","  Downloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (642 bytes)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (5.4.0)\n","Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (3.10.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (3.10.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.9.1)\n","Collecting num2words (from -r requirements.txt (line 9))\n","  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n","Collecting pandas==1.5.3 (from -r requirements.txt (line 11))\n","  Downloading pandas-1.5.3.tar.gz (5.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting plotly_express (from -r requirements.txt (line 12))\n","  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Collecting pyDAWG (from -r requirements.txt (line 13))\n","  Downloading pyDAWG-1.0.1.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyLDAvis (from -r requirements.txt (line 14))\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (1.16.3)\n","Collecting sklearn_crfsuite (from -r requirements.txt (line 17))\n","  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n","Collecting stop_words (from -r requirements.txt (line 18))\n","  Downloading stop_words-2025.11.4-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (2.19.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (4.67.1)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 21)) (1.9.4)\n","Requirement already satisfied: python-dateutil\u003e=2.8.1 in /usr/local/lib/python3.12/dist-packages (from pandas==1.5.3-\u003e-r requirements.txt (line 11)) (2.9.0.post0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==1.5.3-\u003e-r requirements.txt (line 11)) (2025.2)\n","Requirement already satisfied: smart_open\u003e=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim-\u003e-r requirements.txt (line 1)) (7.5.0)\n","Requirement already satisfied: debugpy\u003e=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (1.8.15)\n","Requirement already satisfied: jupyter-client\u003e=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (7.4.9)\n","Requirement already satisfied: matplotlib-inline\u003e=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (0.2.1)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (1.6.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (25.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (5.9.5)\n","Requirement already satisfied: pyzmq\u003e=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (26.2.1)\n","Requirement already satisfied: tornado\u003e=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (6.5.1)\n","Requirement already satisfied: traitlets\u003e=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel-\u003e-r requirements.txt (line 2)) (5.7.1)\n","Requirement already satisfied: setuptools\u003e=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (75.2.0)\n","Collecting jedi\u003e=0.16 (from ipython-\u003e-r requirements.txt (line 3))\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,\u003c3.1.0,\u003e=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (3.0.52)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (2.19.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (0.2.0)\n","Requirement already satisfied: pexpect\u003e4.3 in /usr/local/lib/python3.12/dist-packages (from ipython-\u003e-r requirements.txt (line 3)) (4.9.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras-\u003e-r requirements.txt (line 6)) (1.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras-\u003e-r requirements.txt (line 6)) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras-\u003e-r requirements.txt (line 6)) (0.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras-\u003e-r requirements.txt (line 6)) (3.15.1)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras-\u003e-r requirements.txt (line 6)) (0.17.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras-\u003e-r requirements.txt (line 6)) (0.5.3)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 7)) (1.3.3)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 7)) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 7)) (4.60.1)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 7)) (1.4.9)\n","Requirement already satisfied: pillow\u003e=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 7)) (11.3.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib-\u003e-r requirements.txt (line 7)) (3.2.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk-\u003e-r requirements.txt (line 8)) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk-\u003e-r requirements.txt (line 8)) (1.5.2)\n","Requirement already satisfied: regex\u003e=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk-\u003e-r requirements.txt (line 8)) (2024.11.6)\n","Collecting docopt\u003e=0.6.2 (from num2words-\u003e-r requirements.txt (line 9))\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: plotly\u003e=4.1.0 in /usr/local/lib/python3.12/dist-packages (from plotly_express-\u003e-r requirements.txt (line 12)) (5.24.1)\n","Requirement already satisfied: statsmodels\u003e=0.9.0 in /usr/local/lib/python3.12/dist-packages (from plotly_express-\u003e-r requirements.txt (line 12)) (0.14.5)\n","Requirement already satisfied: patsy\u003e=0.5 in /usr/local/lib/python3.12/dist-packages (from plotly_express-\u003e-r requirements.txt (line 12)) (1.0.2)\n","INFO: pip is looking at multiple versions of pyldavis to determine which version is compatible with other requirements. This could take a while.\n","Collecting pyLDAvis (from -r requirements.txt (line 14))\n","  Downloading pyLDAvis-3.4.0-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from pyLDAvis-\u003e-r requirements.txt (line 14)) (3.1.6)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from pyLDAvis-\u003e-r requirements.txt (line 14)) (2.14.1)\n","Collecting funcy (from pyLDAvis-\u003e-r requirements.txt (line 14))\n","  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: threadpoolctl\u003e=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn-\u003e-r requirements.txt (line 15)) (3.6.0)\n","Collecting python-crfsuite\u003e=0.9.7 (from sklearn_crfsuite-\u003e-r requirements.txt (line 17))\n","  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Requirement already satisfied: tabulate\u003e=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn_crfsuite-\u003e-r requirements.txt (line 17)) (0.9.0)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (1.6.3)\n","Requirement already satisfied: flatbuffers\u003e=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,\u003e=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (0.6.0)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (0.2.0)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (18.1.1)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u003c6.0.0dev,\u003e=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (5.29.5)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (2.32.4)\n","Requirement already satisfied: six\u003e=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (1.17.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (3.2.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (4.15.0)\n","Requirement already satisfied: wrapt\u003e=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (2.0.1)\n","Requirement already satisfied: grpcio\u003c2.0,\u003e=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (1.76.0)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-\u003e-r requirements.txt (line 19)) (2.19.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (0.45.1)\n","Requirement already satisfied: parso\u003c0.9.0,\u003e=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi\u003e=0.16-\u003eipython-\u003e-r requirements.txt (line 3)) (0.8.5)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client\u003e=6.1.12-\u003eipykernel-\u003e-r requirements.txt (line 2)) (0.4)\n","Requirement already satisfied: jupyter-core\u003e=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client\u003e=6.1.12-\u003eipykernel-\u003e-r requirements.txt (line 2)) (5.9.1)\n","Requirement already satisfied: ptyprocess\u003e=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect\u003e4.3-\u003eipython-\u003e-r requirements.txt (line 3)) (0.7.0)\n","Requirement already satisfied: tenacity\u003e=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly\u003e=4.1.0-\u003eplotly_express-\u003e-r requirements.txt (line 12)) (8.5.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,\u003c3.1.0,\u003e=2.0.0-\u003eipython-\u003e-r requirements.txt (line 3)) (0.2.14)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (3.4.4)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (3.11)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (2025.10.5)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (3.10)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (0.7.2)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0-\u003etensorflow-\u003e-r requirements.txt (line 19)) (3.1.3)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003epyLDAvis-\u003e-r requirements.txt (line 14)) (3.0.3)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich-\u003ekeras-\u003e-r requirements.txt (line 6)) (4.0.0)\n","Requirement already satisfied: platformdirs\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core\u003e=4.9.2-\u003ejupyter-client\u003e=6.1.12-\u003eipykernel-\u003e-r requirements.txt (line 2)) (4.5.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich-\u003ekeras-\u003e-r requirements.txt (line 6)) (0.1.2)\n","Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jellyfish-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.5/360.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n","Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n","Downloading stop_words-2025.11.4-py3-none-any.whl (59 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Building wheels for collected packages: pandas, pyDAWG, docopt\n","  Building wheel for pandas (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pandas: filename=pandas-1.5.3-cp312-cp312-linux_x86_64.whl size=41460890 sha256=60a2d66af29cd13792ea8062899d6f9f2d0d47f957029d2ac783eefdf6010fd5\n","  Stored in directory: /root/.cache/pip/wheels/fb/83/18/8e7307aa1185c5498c5490e4d9c8a1732d9f1056e86c3491c6\n","  Building wheel for pyDAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyDAWG: filename=pyDAWG-1.0.1-cp312-cp312-linux_x86_64.whl size=63710 sha256=198e426b1ad8d4ca1ae724751974b9f9dab3ef5af36dfe93e1a851a7cdf2dc60\n","  Stored in directory: /root/.cache/pip/wheels/57/f0/6b/678352fda0b53b13ba0a40e91642a402c3899b0c1aea6ed07e\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=fdcb5fc7ea0a614d0ae9cacb99f60e7c6e651d5e8d220fd02dd9f6a636480de7\n","  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n","Successfully built pandas pyDAWG docopt\n","Installing collected packages: pyDAWG, funcy, docopt, stop_words, python-crfsuite, num2words, jellyfish, jedi, pandas, gensim, sklearn_crfsuite, pyLDAvis, plotly_express\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","dask-cudf-cu12 25.10.0 requires pandas\u003c2.4.0dev0,\u003e=2.0, but you have pandas 1.5.3 which is incompatible.\n","geopandas 1.1.1 requires pandas\u003e=2.0.0, but you have pandas 1.5.3 which is incompatible.\n","cudf-cu12 25.10.0 requires pandas\u003c2.4.0dev0,\u003e=2.0, but you have pandas 1.5.3 which is incompatible.\n","plotnine 0.14.5 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","arviz 0.22.0 requires pandas\u003e=2.1.0, but you have pandas 1.5.3 which is incompatible.\n","mizani 0.13.5 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","xarray 2025.10.1 requires pandas\u003e=2.2, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed docopt-0.6.2 funcy-2.0 gensim-4.4.0 jedi-0.19.2 jellyfish-1.2.1 num2words-0.5.14 pandas-1.5.3 plotly_express-0.4.1 pyDAWG-1.0.1 pyLDAvis-3.4.0 python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0 stop_words-2025.11.4\n","Collecting utils.py\n","  Downloading utils_py-0.3.0.tar.gz (9.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting nose (from utils.py)\n","  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n","Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: utils.py\n","  Building wheel for utils.py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for utils.py: filename=utils_py-0.3.0-py3-none-any.whl size=12317 sha256=a6bb309c829a531a53a916df511b0d8483d0d664ffdd6c69b37b2b27f27a77cb\n","  Stored in directory: /root/.cache/pip/wheels/f0/4e/fd/f9383f23373cb6b6a7ead2accfbf2a4ec11cc223679a8c20f7\n","Successfully built utils.py\n","Installing collected packages: nose, utils.py\n","Successfully installed nose-1.3.7 utils.py-0.3.0\n"]}],"source":["!pip install unzip\n","!pip install -r requirements.txt\n","!pip install utils.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C5buE-1mFn38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  corpusCine.zip\n","replace __MACOSX/._corpusCine? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["!unzip corpusCine.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aPFaq8MQEL4V"},"outputs":[],"source":["import sys\n","sys.path.append('../..')\n"]},{"cell_type":"markdown","metadata":{"id":"P1i0ZW0Qlch3"},"source":["# Lectura de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"syTecDIlV8z6"},"outputs":[],"source":["from utils import load_cinema_reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1wdE_l0yV8z6"},"outputs":[],"source":["# Path al directorio donde tenemos los datasets con las reviews\n","datasets_path = './'\n","corpus_cine_folder = 'corpusCine'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qY40JB2gV8z7"},"outputs":[],"source":["reviews_dict = load_cinema_reviews(datasets_path, corpus_cine_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pKJPvnXDV8z7"},"outputs":[],"source":["review = reviews_dict.get(0).get('review_text')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5tQur5toUE2X"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cada vez me gusta menos el cine de masas. Las peliculas que ven todo el mundo me parecen cada vez mas coñazo y mas insufribles. No se porqué pero siempre el prota es tonto del culo y tiene suerte, y al final de la peli, cuando ha logrado vencer al mal, se convierte en listo, y las chorradas que hacia al comienzo de la pelicula se esfuman como por arte de magia. Se vuelve maduro e inteligente.Esta peli de Spielberg es mas de lo mismo, huir y huir y que no le den ni un solo tiro. Además el cabron ha metido a un par de actores que es como para echarles de comer aparte. La niña, una vieja metida en el cuerpo de una niña, porque solo hay que verle hablar (en version original claro) para darse cuenta que estamos ante uno de los grandes freaks del cine. Se creeran que hace gracia la nena cuando habla igual que su puta madre, pero a mi me causa pavor. Ver a una cria que habla como una persona madura es algo horroroso. Los niños son niños y verlos fuera de su rol asusta.Luego esta el hijo adolescente que tiene el Cruise. Otro subnormal que es para darle de bofetadas hasta que se te vea el hueso a la mano. Fiel reflejo de lo que se denomina manipulacion militar, el chico quiere matar a los bichos sin ningun arma, hale, a lo loco y sin pensar, venga, a saco.Que quereis que os diga, pero a mi eso me parece fanatismo y locura. Que alguien quiera ir a luchar sin medios es ir a una muerte segura, por eso me jode sobremanera que al final de la pelicula aparezca el mongo este y sus abuelos y toda la familia. Se salva todo dios, y encima la vieja aparece en traje de los domingos y toda maquillada.¿Pero que sinsorgada es esta? Solo falta que Cruise vuelva con su exmujer y que el mundo sea mucho mejor. Tranquilos, os cuento el final de la peli pero no pasa nada, todas las pelis acaban de igual manera, y decir eso no resta misterio al bodrio este. Yo no se de que va Spielberg, pero a este paso se va a convertir tan solo en un director que es bueno haciendo efectos especiales, porque lo que es contando historias?¿No os disteis cuenta de que los norteamericanos no saben guardar la compostura? Que todo lo dicen gritando. En momentos dificiles lo mas sensato es tranquilizarse y no discutir por chorradas, y en toda la puta peli no paran de gritar y pegarse entre ellos.Me hace gracia como al comienzo de la peli, cuando el suelo empieza a resquebrajarse, la gente se queda ahí a mirar que pasa. Joder, no se ellos, pero yo ya estaria corriendo como una puta desde hace tiempo, e escondiéndome. Si veo que un bicho esta lanzando rayos y machacando a to dios, me escondo.En fin, que para que seguir hablando de esta mierda, si lo que querian ya lo han conseguido, que es que pagasemos la entrada, por eso creo que va a ir al cine su prima, a partir de ahora me lo bajare todo de internet, que por lo que se ve, la calidad de las pelis es aceptable, y puedo pasarla palante. Hoy en dia no tener un mando con el forward en la mano es morir.\n"]}],"source":["print(review)"]},{"cell_type":"markdown","metadata":{"id":"r_wzsU_sV8z8"},"source":["# NLTK"]},{"cell_type":"markdown","metadata":{"id":"dzWBkAiPV8z-"},"source":["Acrónimo de Natural Language ToolKit, y nombre del paquete para Python.\n","\n","Ofrece multitud de capacidades para tareas de NLP como pueden ser: tokenizar, stemming, parsing, etc.\n","\n","Documentación: https://www.nltk.org/\n","Libro https://www.nltk.org/book/"]},{"cell_type":"markdown","metadata":{"id":"9ZyuBCVWV8z-"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWLrVaatD19E"},"outputs":[],"source":["!pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"kbDVraybV8z-"},"source":["### A nivel de frase"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCxpkQQLUIUd"},"outputs":[],"source":["import nltk\n","\n","\n","#descargamos un tokenizador de frases\n","nltk.download('punkt_tab')  # Download the punkt_tab model\n","\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"markdown","metadata":{"id":"fwG8vsd5V8z_"},"source":["NLTK dispone de distintos tokenizers:\n","\n","- **TreebankWordTokenizer**: tokenizer por defecto.\n","- **PunktTokenizer**: tokeniza en los signos de puntuación pero los mantiene junto a la palabra.\n","- **WordPunctTokenizer**: separa en los signos de puntuación pero los incluye en tokens separados.\n","- **RegexpTokenizer**: tokenizer que permite trabajar con regex."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQ0enSnfX5Jb"},"outputs":[],"source":["\n","for idx, sent in enumerate(sent_tokenize(review)):\n","    print('Frase {0:10}{1:20}'.format(str(idx), sent))"]},{"cell_type":"markdown","metadata":{"id":"TG4VbB_bV8z_"},"source":["### A nivel de palabra"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gTTThSMV8z_"},"outputs":[],"source":["from nltk import word_tokenize, TreebankWordTokenizer, RegexpTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdoQCNRhV8z_"},"outputs":[],"source":["sentences = nltk.sent_tokenize(review)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ei9l012iUQ19"},"outputs":[],"source":["for sent in sentences:\n","    for idx, word in enumerate(word_tokenize(sent)):\n","        print('Palabra {0:10}{1:20}'.format(str(idx), word))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxtqRryhUUMv"},"outputs":[],"source":["tokenizer = TreebankWordTokenizer()\n","for sent in sentences:\n","    for idx, word in enumerate(tokenizer.tokenize(sent)):\n","        print('Palabra {0:10}{1:20}'.format(str(idx), word))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8iFFn22UZbC"},"outputs":[],"source":["words = list()\n","tokenizer = RegexpTokenizer(r'\\w+')\n","for sent in sentences:\n","    for idx, word in enumerate(tokenizer.tokenize(sent)):\n","        print('Palabra {0:10}{1:20}'.format(str(idx), word))\n","        words.append(word)"]},{"cell_type":"markdown","metadata":{"id":"TwQiv5K_V80A"},"source":["## Stop words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7-6EdfmV80A"},"outputs":[],"source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1nKPiaXrlom"},"outputs":[],"source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E88s9Ry0V80A"},"outputs":[],"source":["nltk_sw_list = stopwords.words('spanish')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LYl7AnfEqPW"},"outputs":[],"source":["if 'no' in nltk_sw_list:\n","  print(True)"]},{"cell_type":"markdown","metadata":{"id":"v2zaxqBZV80A"},"source":["## Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwrORY1zV80A"},"outputs":[],"source":["from nltk.stem.snowball import SpanishStemmer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqE2Y4qcV80A"},"outputs":[],"source":["stemmer = SpanishStemmer(ignore_stopwords=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QXbXHBLFAZY"},"outputs":[],"source":["print('{0:15}{1:10}'.format('Token' ,'Stem'))\n","for word in words:\n","    print('{0:15}{1:10}'.format(word, stemmer.stem(word)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTTeKWlWVYLf"},"outputs":[],"source":["print('{0:15}{1:10}'.format('Token' ,'Stem'))\n","for word in words:\n","    print('{0:15}{1:10}'.format(word, stemmer.stem(word)))"]},{"cell_type":"markdown","metadata":{"id":"QIG_udZmV80B"},"source":["## Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"uEHHO__DV80B"},"source":["NLTK implementa el WordNetLemmatizer que hace uso de WordNet para la búsqueda (lookup) de lemmas.\n","\n","Es necesario descargar previamente el corpora de WordNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Szp-xqMvV80B"},"outputs":[],"source":["nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBKmwVznV80B"},"outputs":[],"source":["import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ta3vru57V80B"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7eUpBevVcCY"},"outputs":[],"source":["print('{0:15}{1:10}'.format('Token' ,'Lemma'))\n","for word in words:\n","    print('{0:15}{1:10}'.format(word, lemmatizer.lemmatize(word)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_L0jlDoV80B"},"outputs":[],"source":["lemmatizer.lemmatize('casas')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYJOstseV80B"},"outputs":[],"source":["lemmatizer.lemmatize('houses')"]},{"cell_type":"markdown","metadata":{"id":"2C-HiM6RV80B"},"source":["# Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YR0RrYS0Vg5o"},"outputs":[],"source":["!pip install num2words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjCX0DoOV80B"},"outputs":[],"source":["import unicodedata\n","from num2words import num2words\n","\n","from nltk import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","def nltk_cleaner(text, tokenizer, sw_list, lemmatizer):\n","    clean_text = list()\n","\n","    # Eliminar acentos, etc\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","\n","    # Separar palabras eliminando signos de puntuación\n","    for word in tokenizer.tokenize(text):\n","\n","        # Eliminar stop words\n","        if word not in sw_list:\n","\n","        # Eliminar espacios sobrantes, convertir a minúsculas y lematizar\n","            clean_word = lemmatizer.lemmatize(word).lower().strip()\n","\n","        # Convertir dígitos a palabras\n","            if clean_word.isdigit():\n","                clean_word = num2words(clean_word, lang='es')\n","\n","            clean_text.append(clean_word)\n","\n","    return ' '.join(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjtZVosoV80C"},"outputs":[],"source":["tokenizer = RegexpTokenizer(r'\\w+')\n","sw_list = stopwords.words('spanish')\n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNPdjIgcVlsw"},"outputs":[],"source":["nltk_cleaner(review, tokenizer, sw_list, lemmatizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoZz323PV80C"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMiDAJAOV80C"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}