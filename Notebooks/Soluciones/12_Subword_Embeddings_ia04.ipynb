{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJBuKgYggV6W"
      },
      "source": [
        "# FastText\n",
        "\n",
        "A diferencia de Word2Vec, que trabaja a nivel de palabra, FastText trata de capturar la información morfológica de las palabras.\n",
        "\n",
        ">*\"[...] we propose a new approach **based on the skipgram model, where each word is represented as a bag of character n-grams**. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. [...]\"* <br>(Mikolov et al., Enriching Word Vectors with Subword Information, https://arxiv.org/pdf/1607.04606.pdf)\n",
        "\n",
        "De esta manera, una palabra quedará representada por sus n-grams.\n",
        "\n",
        "El tamaño de los n-grams deberá ser definido como hiperparámetro\n",
        "- min_n: valor mínimo de _n_ a considerar\n",
        "- max_n: valor máximo de _n_ a considerar\n",
        "\n",
        "Ejemplo:\n",
        ">*\"Me gusta el procesado del lenguaje natural\"*\n",
        ">* Ejemplo de *skip-gram* pre-procesado con una ventana de contexto de 2 palabras\n",
        ">\n",
        ">$w_{target} =$ \"procesado\" &emsp;$w_{context} =$ [\"gusta\", \"el\", \"del\", \"lenguaje\"]\n",
        ">\n",
        ">     (\"procesado\", \"gusta\")\n",
        ">\n",
        "> Descomoposición de n-grams con min_n=3 and max_n=4:\n",
        ">\n",
        ">\"procesado\" = [\"$<$pr\", \"pro\", ..., \"ado\", \"do$>$\", \"$<$pro\", \"roce\", ..., \"sado\", \"ado$>$\"]\n",
        ">\n",
        ">* De este modo, la similitud será: <br><br>\n",
        ">&emsp;$\\boxed{s(w_{target}, w_{context}) = \\sum_{g \\in G_{w_{target}}}z_{g}^T v_{w_{context}}}$, where $G_{w_{target}}\\subset\\{g_{1}, ..., g_{G}\\}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0qLjFS_gV6a"
      },
      "source": [
        "## Palabras más similares"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim spacy numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O07e4TpkMEgn",
        "outputId": "5766e27e-c381-4276-e4d3-858db5ef6c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (27 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.8.11-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (33.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.2/33.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, gensim, spacy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.4.0 numpy-2.3.5 spacy-3.8.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5ca0a02001ca4982a4a3c3462c58fbcb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTzIvoQ-gV6b"
      },
      "outputs": [],
      "source": [
        "def print_sim_words(word, model1, model2):\n",
        "    query = \"Most similar to {}\".format(word)\n",
        "    print(query)\n",
        "    print(\"-\"*len(query))\n",
        "    for (sim1, sim2) in zip(model1.wv.most_similar(word), model2.wv.most_similar(word)):\n",
        "        print(\"{}:{}{:.3f}{}{}:{}{:.3f}\".format(sim1[0],\n",
        "                                               \" \"*(20-len(sim1[0])),\n",
        "                                               sim1[1],\n",
        "                                               \" \"*10,\n",
        "                                               sim2[0],\n",
        "                                               \" \"*(20-len(sim2[0])),\n",
        "                                               sim2[1]))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GYhLbgugV6c"
      },
      "source": [
        "## Importamos las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD5YKchbgV6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from gensim.models import FastText\n",
        "from gensim.models.word2vec import LineSentence\n",
        "from gensim.models.phrases import Phrases, Phraser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX8m7DX1gV6c"
      },
      "source": [
        "## Lectura de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_XwCiBWEqXU",
        "outputId": "6e07a61f-92b7-40ad-9b25-816bf962428f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unzip\n",
            "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unzip\n",
            "  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1281 sha256=773b039b64e42c6ee3a2eff477c52b06f985c801aaa42e0cfd5538deaf484591\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/5b/81/0f3e1e533b52883f88ab978178c15627a4fce4c13f74911dce\n",
            "Successfully built unzip\n",
            "Installing collected packages: unzip\n",
            "Successfully installed unzip-1.0.0\n",
            "Archive:  df_clean_simpsons.csv.zip\n",
            "  inflating: df_clean_simpsons.csv   \n",
            "  inflating: __MACOSX/._df_clean_simpsons.csv  \n"
          ]
        }
      ],
      "source": [
        "!pip install unzip\n",
        "!unzip df_clean_simpsons.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXn1Q5bgOKPT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_clean = pd.read_csv('./df_clean_simpsons.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1GDH8oeOd5G"
      },
      "outputs": [],
      "source": [
        "\n",
        "sent = [row.split() for row in df_clean['clean']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzpsA8BpgV6d"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsUJBv_igV6d"
      },
      "outputs": [],
      "source": [
        "sg_params = {\n",
        "    'sg': 1,\n",
        "    'vector_size': 300,\n",
        "    'min_count': 5,\n",
        "    'window': 5,\n",
        "    'hs': 0,\n",
        "    'negative': 20,\n",
        "    'workers': 4,\n",
        "    'min_n': 3,\n",
        "    'max_n': 6\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT5AbNulgV6d"
      },
      "source": [
        "## Inicializamos el objeto FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_uqb5bjggc4",
        "outputId": "2fa61b7b-3808-41fc-a9bf-905f02596513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class FastText in module gensim.models.fasttext:\n",
            "\n",
            "class FastText(gensim.models.word2vec.Word2Vec)\n",
            " |  FastText(sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      FastText\n",
            " |      gensim.models.word2vec.Word2Vec\n",
            " |      gensim.utils.SaveLoad\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
            " |      Train, use and evaluate word representations learned using the method\n",
            " |      described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_,\n",
            " |      aka FastText.\n",
            " |\n",
            " |      The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n",
            " |      :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the\n",
            " |      original Fasttext implementation via :func:`~gensim.models.fasttext.load_facebook_model`.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str, optional\n",
            " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus'\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such\n",
            " |          examples. If you don't supply `sentences`, the model is left uninitialized -- use if you plan to\n",
            " |          initialize it in some other way.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left\n",
            " |          uninitialized).\n",
            " |      min_count : int, optional\n",
            " |          The model ignores all words with total frequency lower than this.\n",
            " |      vector_size : int, optional\n",
            " |          Dimensionality of the word vectors.\n",
            " |      window : int, optional\n",
            " |          The maximum distance between the current and predicted word within a sentence.\n",
            " |      workers : int, optional\n",
            " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
            " |      alpha : float, optional\n",
            " |          The initial learning rate.\n",
            " |      min_alpha : float, optional\n",
            " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
            " |      sg : {1, 0}, optional\n",
            " |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n",
            " |      hs : {1,0}, optional\n",
            " |          If 1, hierarchical softmax will be used for model training.\n",
            " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
            " |      seed : int, optional\n",
            " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
            " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
            " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
            " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
            " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
            " |      max_vocab_size : int, optional\n",
            " |          Limits the RAM during vocabulary building; if there are more unique\n",
            " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
            " |          Set to `None` for no limit.\n",
            " |      sample : float, optional\n",
            " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
            " |          useful range is (0, 1e-5).\n",
            " |      negative : int, optional\n",
            " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
            " |          should be drawn (usually between 5-20).\n",
            " |          If set to 0, no negative sampling is used.\n",
            " |      ns_exponent : float, optional\n",
            " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
            " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
            " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
            " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
            " |          other values may perform better for recommendation applications.\n",
            " |      cbow_mean : {1,0}, optional\n",
            " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
            " |      hashfxn : function, optional\n",
            " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
            " |      iter : int, optional\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during\n",
            " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n",
            " |\n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |\n",
            " |      sorted_vocab : {1,0}, optional\n",
            " |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
            " |      batch_words : int, optional\n",
            " |          Target size (in words) for batches of examples passed to worker threads (and\n",
            " |          thus cython routines).(Larger batches will be passed if individual\n",
            " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
            " |      min_n : int, optional\n",
            " |          Minimum length of char n-grams to be used for training word representations.\n",
            " |      max_n : int, optional\n",
            " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
            " |          lesser than `min_n` to avoid char ngrams being used.\n",
            " |      word_ngrams : int, optional\n",
            " |          In Facebook's FastText, \"max length of word ngram\" - but gensim only supports the\n",
            " |          default of 1 (regular unigram word handling).\n",
            " |      bucket : int, optional\n",
            " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
            " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
            " |          The default value of 2000000 consumes as much memory as having 2000000 more in-vocabulary\n",
            " |          words in your model.\n",
            " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          List of callbacks that need to be executed/run at specific stages during training.\n",
            " |      max_final_vocab : int, optional\n",
            " |          Limits the vocab to a target vocab size by automatically selecting\n",
            " |          ``min_count```.  If the specified ``min_count`` is more than the\n",
            " |          automatically calculated ``min_count``, the former will be used.\n",
            " |          Set to ``None`` if not required.\n",
            " |      shrink_windows : bool, optional\n",
            " |          New in 4.1. Experimental.\n",
            " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
            " |          for each target word during training, to match the original word2vec algorithm's\n",
            " |          approximate weighting of context words by distance. Otherwise, the effective\n",
            " |          window size is always fixed to `window` words to either side.\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      Initialize and train a `FastText` model:\n",
            " |\n",
            " |      .. sourcecode:: pycon\n",
            " |\n",
            " |          >>> from gensim.models import FastText\n",
            " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |          >>>\n",
            " |          >>> model = FastText(sentences, min_count=1)\n",
            " |          >>> say_vector = model.wv['say']  # get vector for word\n",
            " |          >>> of_vector = model.wv['of']  # get vector for out-of-vocab word\n",
            " |\n",
            " |      Attributes\n",
            " |      ----------\n",
            " |      wv : :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
            " |          This object essentially contains the mapping between words and embeddings. These are similar to\n",
            " |          the embedding computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also\n",
            " |          include vectors for n-grams. This allows the model to compute embeddings even for **unseen**\n",
            " |          words (that do not exist in the vocabulary), as the aggregate of the n-grams included in the word.\n",
            " |          After training the model, this attribute can be used directly to query those embeddings in various\n",
            " |          ways. Check the module level docstring for some examples.\n",
            " |\n",
            " |  estimate_memory(self, vocab_size=None, report=None)\n",
            " |      Estimate memory that will be needed to train a model, and print the estimates to log.\n",
            " |\n",
            " |  init_sims(self, replace=False)\n",
            " |      Precompute L2-normalized vectors. Obsoleted.\n",
            " |\n",
            " |      If you need a single unit-normalized vector for some key, call\n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
            " |      ``fasttext_model.wv.get_vector(key, norm=True)``.\n",
            " |\n",
            " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
            " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      replace : bool\n",
            " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
            " |          You lose information if you do this.\n",
            " |\n",
            " |  load_binary_data(self, encoding='utf8')\n",
            " |      Load data from a binary file created by Facebook's native FastText.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      encoding : str, optional\n",
            " |          Specifies the encoding.\n",
            " |\n",
            " |  save(self, *args, **kwargs)\n",
            " |      Save the Fasttext model. This saved model can be loaded again using\n",
            " |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n",
            " |      and getting vectors for out-of-vocabulary words.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Store the model to this file.\n",
            " |\n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.fasttext.FastText.load`\n",
            " |          Load :class:`~gensim.models.fasttext.FastText` model.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |\n",
            " |  load(*args, **kwargs)\n",
            " |      Load a previously saved `FastText` model.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the saved file.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`~gensim.models.fasttext.FastText`\n",
            " |          Loaded model.\n",
            " |\n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.fasttext.FastText.save`\n",
            " |          Save :class:`~gensim.models.fasttext.FastText` model.\n",
            " |\n",
            " |  load_fasttext_format(model_file, encoding='utf8')\n",
            " |      Deprecated.\n",
            " |\n",
            " |      Use :func:`gensim.models.fasttext.load_facebook_model` or\n",
            " |      :func:`gensim.models.fasttext.load_facebook_vectors` instead.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.models.word2vec.Word2Vec:\n",
            " |\n",
            " |  __str__(self)\n",
            " |      Human readable representation of the model's state.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      str\n",
            " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
            " |          and learning rate.\n",
            " |\n",
            " |  add_null_word(self)\n",
            " |\n",
            " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
            " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      corpus_iterable : iterable of list of str\n",
            " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      update : bool\n",
            " |          If true, the new words in `sentences` will be added to model's vocab.\n",
            " |      progress_per : int, optional\n",
            " |          Indicates how many words to process before showing/updating the progress.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |\n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |\n",
            " |      **kwargs : object\n",
            " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
            " |\n",
            " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
            " |      Build vocabulary from a dictionary of word frequencies.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      word_freq : dict of (str, int)\n",
            " |          A mapping from a word in the vocabulary to its frequency count.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
            " |      corpus_count : int, optional\n",
            " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |\n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |\n",
            " |      update : bool, optional\n",
            " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
            " |\n",
            " |  create_binary_tree(self)\n",
            " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
            " |      word counts. Frequent words will have shorter binary codes.\n",
            " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
            " |\n",
            " |  get_latest_training_loss(self)\n",
            " |      Get current value of the training loss.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Current training loss.\n",
            " |\n",
            " |  init_weights(self)\n",
            " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
            " |\n",
            " |  make_cum_table(self, domain=2147483647)\n",
            " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
            " |      drawing random words in the negative-sampling training routines.\n",
            " |\n",
            " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
            " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
            " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
            " |\n",
            " |  predict_output_word(self, context_words_list, topn=10)\n",
            " |      Get the probability distribution of the center word given context words.\n",
            " |\n",
            " |      Note this performs a CBOW-style propagation, even in SG models,\n",
            " |      and doesn't quite weight the surrounding words the same as in\n",
            " |      training -- so it's just one crude way of using a trained model\n",
            " |      as a predictor.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      context_words_list : list of (str and/or int)\n",
            " |          List of context words, which may be words themselves (str)\n",
            " |          or their index in `self.wv.vectors` (int).\n",
            " |      topn : int, optional\n",
            " |          Return `topn` words and their probabilities.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float)\n",
            " |          `topn` length list of tuples of (word, probability).\n",
            " |\n",
            " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
            " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
            " |      and `sample` (controlling the downsampling of more-frequent words).\n",
            " |\n",
            " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
            " |      report the size of the retained vocabulary, effective corpus length, and\n",
            " |      estimated memory requirements. Results are both printed via logging and\n",
            " |      returned as a dict.\n",
            " |\n",
            " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
            " |      unless `keep_raw_vocab` is set.\n",
            " |\n",
            " |  prepare_weights(self, update=False)\n",
            " |      Build tables and model weights based on final vocabulary settings.\n",
            " |\n",
            " |  reset_from(self, other_model)\n",
            " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
            " |\n",
            " |      Structures copied are:\n",
            " |          * Vocabulary\n",
            " |          * Index to word mapping\n",
            " |          * Cumulative frequency table (used for negative sampling)\n",
            " |          * Cached corpus length\n",
            " |\n",
            " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
            " |      then share all vocabulary-related structures other than vectors, neither should then\n",
            " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
            " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
            " |\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
            " |          Another model to copy the internal structures from.\n",
            " |\n",
            " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
            " |\n",
            " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
            " |      Score the log probability for a sequence of sentences.\n",
            " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
            " |\n",
            " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
            " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
            " |\n",
            " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
            " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
            " |\n",
            " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
            " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
            " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
            " |      how to use such scores in document classification.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |      total_sentences : int, optional\n",
            " |          Count of sentences.\n",
            " |      chunksize : int, optional\n",
            " |          Chunksize of jobs\n",
            " |      queue_factor : int, optional\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float, optional\n",
            " |          Seconds to wait before reporting progress.\n",
            " |\n",
            " |  seeded_vector(self, seed_string, vector_size)\n",
            " |\n",
            " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
            " |      Update the model's neural weights from a sequence of sentences.\n",
            " |\n",
            " |      Notes\n",
            " |      -----\n",
            " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
            " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
            " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
            " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
            " |      you can simply use `total_examples=self.corpus_count`.\n",
            " |\n",
            " |      Warnings\n",
            " |      --------\n",
            " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
            " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
            " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      corpus_iterable : iterable of list of str\n",
            " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |          See also the `tutorial on data streaming in Python\n",
            " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      total_examples : int\n",
            " |          Count of sentences.\n",
            " |      total_words : int\n",
            " |          Count of raw words in sentences.\n",
            " |      epochs : int\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      start_alpha : float, optional\n",
            " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
            " |          for this one call to`train()`.\n",
            " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
            " |          (not recommended).\n",
            " |      end_alpha : float, optional\n",
            " |          Final learning rate. Drops linearly from `start_alpha`.\n",
            " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
            " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
            " |          (not recommended).\n",
            " |      word_count : int, optional\n",
            " |          Count of words already trained. Set this to 0 for the usual\n",
            " |          case of training on all words in sentences.\n",
            " |      queue_factor : int, optional\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float, optional\n",
            " |          Seconds to wait before reporting progress.\n",
            " |      compute_loss: bool, optional\n",
            " |          If True, computes and stores loss value which can be retrieved using\n",
            " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
            " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          Sequence of callbacks to be executed at specific stages during training.\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      .. sourcecode:: pycon\n",
            " |\n",
            " |          >>> from gensim.models import Word2Vec\n",
            " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |          >>>\n",
            " |          >>> model = Word2Vec(min_count=1)\n",
            " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
            " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
            " |          (1, 30)\n",
            " |\n",
            " |  update_weights(self)\n",
            " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.utils.SaveLoad:\n",
            " |\n",
            " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
            " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
            " |      optionally log the event at `log_level`.\n",
            " |\n",
            " |      Events are important moments during the object's life, such as \"model created\",\n",
            " |      \"model saved\", \"model loaded\", etc.\n",
            " |\n",
            " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
            " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
            " |      but is useful during debugging and support.\n",
            " |\n",
            " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
            " |      will not record events into `self.lifecycle_events` then.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      event_name : str\n",
            " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
            " |      event : dict\n",
            " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
            " |          Can be empty.\n",
            " |\n",
            " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
            " |\n",
            " |          - `datetime`: the current date & time\n",
            " |          - `gensim`: the current Gensim version\n",
            " |          - `python`: the current Python version\n",
            " |          - `platform`: the current platform\n",
            " |          - `event`: the name of this event\n",
            " |      log_level : int\n",
            " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(FastText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw9JpI6AgV6e"
      },
      "outputs": [],
      "source": [
        "# Skip Gram\n",
        "ft_sg = FastText(**sg_params)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVumppbugV6e"
      },
      "source": [
        "## Construímos el vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEahkKoUgV6e"
      },
      "outputs": [],
      "source": [
        "# Skip Gram\n",
        "ft_sg.build_vocab(sent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfwtYvJoQgGi",
        "outputId": "af482846-6e6f-41fd-f48d-059e9370f670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario compuesto por 8770 palabras\n"
          ]
        }
      ],
      "source": [
        "print('Vocabulario compuesto por {} palabras'.format(len(ft_sg.wv.key_to_index)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlXG1UfHgV6f"
      },
      "source": [
        "## Entrenamos los pesos de los embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz1U_c4pgV6f",
        "outputId": "ff718064-c8f1-44a5-c965-908cf9b920d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9125060, 10741780)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Skip Gram\n",
        "\n",
        "\n",
        "ft_sg.train(sent, total_examples=len(sent), epochs=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F62S0H2gV6f"
      },
      "source": [
        "## Guardamos los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTt2zNitgV6f"
      },
      "outputs": [],
      "source": [
        "ft_sg.save('./w2v_model_fast.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-79KWIfjgV6f"
      },
      "source": [
        "## Algunos resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "878nHw2fQjPM",
        "outputId": "dd8c3546-c700-4985-fce3-eb9c23145e47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('knockahomer', 0.6395146250724792),\n",
              " ('homey', 0.6154800653457642),\n",
              " ('homeboy', 0.5624999403953552),\n",
              " ('hom', 0.5478023290634155),\n",
              " ('astronomer', 0.5175456404685974),\n",
              " ('hometown', 0.5051910877227783),\n",
              " ('fonzie', 0.4758222699165344),\n",
              " ('homosexual', 0.47149208188056946),\n",
              " ('home', 0.4672473669052124),\n",
              " ('thompson', 0.4571842849254608)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "ft_sg.wv.most_similar(positive=[\"homer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6IPOQj_QjRY",
        "outputId": "845b8ee1-7f2a-4396-e662-e401c1174015"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sarge', 0.654728889465332),\n",
              " ('margie', 0.6150627732276917),\n",
              " ('margarita', 0.6041011214256287),\n",
              " ('margaret', 0.5914472341537476),\n",
              " ('barge', 0.5579581260681152),\n",
              " ('marmaduke', 0.5227359533309937),\n",
              " ('marjorie', 0.5183671712875366),\n",
              " ('marlon', 0.4624136686325073),\n",
              " ('marco', 0.460935115814209),\n",
              " ('mars', 0.45457586646080017)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ft_sg.wv.most_similar(positive=[\"marge\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "579VgWJHQpgG",
        "outputId": "a16b8e05-4ef5-4415-f935-5e045d9739e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barto', 0.5829876065254211),\n",
              " ('bartman', 0.520098865032196),\n",
              " ('baryshnikov', 0.4961232542991638),\n",
              " ('bartron', 0.4920521676540375),\n",
              " ('barty', 0.47650855779647827),\n",
              " ('barf', 0.471396267414093),\n",
              " ('dart', 0.47074419260025024),\n",
              " ('bartholomew', 0.4664735198020935),\n",
              " ('impart', 0.46606168150901794),\n",
              " ('nikki', 0.46264785528182983)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "ft_sg.wv.most_similar(positive=[\"bart\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATqRO5LoQsLz",
        "outputId": "9fd6c143-4bb3-4c58-8d16-8e377687b1dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.32119647)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "ft_sg.wv.similarity('maggie', 'baby')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXCHTWzKQvUd",
        "outputId": "17a1f60c-8a19-47c8-94d8-d885351e6e19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(0.28339562)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "ft_sg.wv.similarity('bart', 'nelson')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "os5efiscQxk5",
        "outputId": "810ddabe-d2d2-47ed-aa69-5f87a6d14f3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'milhouse'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "ft_sg.wv.doesnt_match(['jimbo', 'milhouse', 'kearney'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vu_WWM9mQ0TO",
        "outputId": "33684d94-7044-442e-874d-7633cb1d9365"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'homer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "ft_sg.wv.doesnt_match(['homer', 'patty', 'selma'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uivGUy4AgV6g"
      },
      "source": [
        "## Out-of-Vocabulary (OOV) Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enFbsjjNgV6g"
      },
      "source": [
        "la cantidad de n-grams creados durante el entrenamiento del FastText hace improbable (que no imposible) que alguna palabra no pueda ser construída como una bolsa de n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4rs_XO7Q2wX",
        "outputId": "427f3a73-6f26-406f-ea22-48e3f3e51cda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "'asereje' in ft_sg.wv.key_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzL_dhM-Q5j8",
        "outputId": "1f7215db-724c-4707-a554-5977f509a646"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('taser', 0.6168254613876343),\n",
              " ('eraser', 0.6077090501785278),\n",
              " ('serenity', 0.598382830619812),\n",
              " ('phaser', 0.5907742381095886),\n",
              " ('laser', 0.5797291398048401),\n",
              " ('heeere', 0.5598991513252258),\n",
              " ('sera', 0.5534683465957642),\n",
              " ('analysis', 0.5377046465873718),\n",
              " ('liser', 0.5299598574638367),\n",
              " ('derriere', 0.5272972583770752)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "ft_sg.wv.most_similar('asereje')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA7HOBBCQ8Ux",
        "outputId": "bc04383e-71c8-49d3-9719-7831a2f15b69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "ft_sg.wv['asereje'].shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}